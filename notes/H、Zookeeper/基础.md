# 一、WARO协议

一个简单的副本控制协议。

客户端向任一副本写数据时，需要所有副本全部同步完成后才视为更新成功。这样下次只需读任一一个副本的数据即可，保证了强一致性。

缺点：1.一个副本失败即视为整个写操作失败，可用性较低

​           2.需要等待全部副本更新完成。延时高，可用性低

# 二、Quorum

> 假设有 N 个副本，更新操作 wi 在 W 个副本中更新成功之后，则认为此次更新操作 wi 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读 R 个副本，其中，W+R>N ，即 W 和 R 有重叠，一般，W+R=N+1。
>
> N = 存储数据副本的数量
> W = 更新成功所需的副本
> R = 一次数据对象读取要访问的副本的数量

听起来有些抽象，举个例子：

假设我有5个副本，更新操作成功写入了3个，另外2个副本仍是旧数据，此时在读取的时候，只要确保读取副本的数量大于2，那么肯定就会读到最新的数据。至于如何确定哪份数据是最新的，我们可以通过引入数据版本号的方式判断（Quorum 机制的使用需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据）

 **应用**

​    **1. Zookeeper**

​		其选举机制遵循了Quorum机制，超过半数则成功。要求集群节点个数为奇数也是基于这个原因：

​		1.奇数个能保证选举不会出现平票，避免脑裂。

​		2.Leader向Follower同步数据时，超过半数的Follower同步成功，才会认为数据写入成功。

2. **Redis的哨兵（sentinel）机制**

# 三、Paxos算法

https://www.cnblogs.com/linbingdong/p/6253479.html

> Paxos是基于**消息传递**且具有**高度容错性**的一致性算法，是目前公认解决分布式一致性问题最有效的算法之一。

## 3.1 背景

在常见的分布式系统中，总会发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对**某个数据的值**达成**一致**，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。

## 3.2 相关概念

- **Proposer**
- **Acceptor**
- **Learners**

在具体的实现中，一个进程可能**同时充当多种角色**。比如一个进程可能**既是Proposer又是Acceptor又是Learner**。

还有一个很重要的概念叫**提案（Proposal）**。最终要达成一致的value就在提案里。

Proposer可以提出（propose）提案；Acceptor可以接受（accept）提案；如果某个提案被选定（chosen），那么该提案里的value就被选定了。

**确定提案被chosen：**

1. Proposer提的提案被Acceptor

 	2. Acceptor接受了某个提案
 	3. Learner：Acceptor告诉哪个value被选定

## 3.3 推导过程

### 3.3.1 最简单的方案---只有一个Acceptor

<img src="E:\workspace\github\book-mark\images\zookeeper01.png" alt="只有一个Acceptor" style="zoom:50%;" />

### 3.3.2 多个Acceptor

#### 3.3.2.1 约束

>P1: 一个Acceptor必须接收它收到的第一个提案

​	只遵循P1，不同的Proposal向不同的Acceptor提案，并且每个提案的value都不同，导致多个不同的value被选定

> 规定：一个提案被接受需要**半数以上**的Acceptor接受

​	这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图的情况。v1、v2、v3都没有被选定，因为它	们都只被一个Acceptor的接受。

>P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。
>P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。
>
>只要满足了P2a，就能满足P2。

​	问题：

​	有5个Acceptor，Proposer1提出[N1,V1]的提案，Acceptor1-4（超过半数）均接受了该提案，对于Acceptor1-4来说，他们都认为V1被选定。
Acceptor5刚刚从宕机状态恢复过来（之前Acceptor5没有收到过任何提案），此时Proposer2向Acceptor5发送了[N2,V2]的提案（V2≠V1且N2>N1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案！同时Acceptor1认为V2被选定。

​	出现问题：		

1. Acceptor1-4认为V1被选定，Acceptor5和Proposer2认为V2被选定。出现了不一致。
2. V1被选定了，但是编号更高的被Acceptor5接受的提案[N2,V2]的value为V2，且V2≠V1。这就跟P2a（如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v）矛盾了。

> P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。

那么，如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？

> P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个：
>
> - S中每个Acceptor都没有接受过编号小于N的提案。
> - S中Acceptor接受过的最大编号的提案的value为V。

#### 3.3.2.2 Proposer生成提案

为了满足P2b，Proposer生成提案前，应该先去**『学习』**已经被选定或者可能被选定的value，然后以该value作为自己提出的提案的value。

如果没有value被选定，Proposer才可以自己决定value的值。这样才能达成一致。这个学习的阶段是通过一个**『Prepare请求』**实现的。

**提案生成算法**：

​	 Proposer选择一个**新的提案编号N**，然后向某个Acceptor集合（半数以上）发送请求，要求该集合中的每个Acceptor做出如下响应（response：

​				1. Acceptor不再接受提案编号<N的提案

​				2. 如果Acceptor已经接受过提案，那就向Proposer响应已经接受过的编号中<N的最大编号的提案

​    如果Porposer接受到半数以上Acceptor的响应，从这些响应的提案中选择编号最大的提案，根据这个提案生成[N,V]的提案，如果所有的响应中都没有提案，那么这个V可以由Proposer自己决定。

Proposer将该**提案**发送给**半数以上**的Acceptor集合，并期望这些Acceptor能接受该提案

#### 3.3.2.3 Acceptor接受提案

我们对Acceptor接受提案给出如下约束：

> P1a：一个Acceptor只要尚**未响应过**任何**编号大于N**的**Prepare请求**，那么他就可以**接受**这个**编号为N的提案**。

## 3.4 算法整体描述

Paxos算法分为**两个阶段**。

### 3.4.1 阶段一

(a) Proposer选择一个**提案编号N**，然后向**半数以上**的Acceptor发送编号为N的**Prepare请求**。

(b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N**大于**该Acceptor已经**响应过的**所有**Prepare请求**的编号，那么它就会将它已经**接受过的编号最大的提案（如果有的话）**作为响应反馈给Proposer，同时该Acceptor承诺**不再接受**任何**编号小于N的提案**。

### 3.4.2 阶段二

(a) 如果Proposer收到**半数以上**Acceptor对其发出的编号为N的Prepare请求的**响应**，那么它就会发送一个针对**[N,V]提案**的**Accept请求**给**半数以上**的Acceptor。注意：V就是收到的**响应**中**编号最大的提案的value**，如果响应中**不包含任何提案**，那么V就由Proposer**自己决定**。

(b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor**没有**对编号**大于N**的**Prepare请求**做出过**响应**，它就**接受该提案**。

![Paxos算法流程](E:\workspace\github\book-mark\images\zookeeper02.png)

## 3.5 Learner学习被选定的value

<img src="E:\workspace\github\book-mark\images\zookeeper03.png" alt="幻灯片17.png" style="zoom:80%;" />

## 3.6 活锁问题

![image-20210224152053848](E:\workspace\github\book-mark\images\zookeeper04.png)

通过选取**主Proposer**，就可以保证Paxos算法的活性。至此，我们得到一个**既能保证安全性，又能保证活性**的**分布式一致性算法**——**Paxos算法**。

# 四、ZAB协议

https://www.cnblogs.com/stateis0/p/9062133.html

由于Paxos算法实现起来较难，而且存在活锁和全序问题，一次Zookeeper并没有采用Paxos算法，而是采用了ZAB协议。

ZAB（zookeeper atomic broadcast）是一种支持崩溃恢复的原子广播协议，基于Fast Paxos实现。

zookeeper使用单一主进程Leader处理客户端所有事务请求（写请求）。集群采用原子广播协议，以事务提交proposal的形式广播到所有的副本进程。每一个事务分配一个全局递增的事务编号xid。

若客户端向Follower节点发起写请求，Follower会把该请求转发给Leader，Leader在向所有Follower广播该请求，如果超过半数节点同意写请求，则写请求就会提交，Leader通知所有的订阅者同步数据。

客户端发起读请求，由接收的节点根据自己保存的数据响应。

# 五、Zookeeper

Zookeeper 是一个为分布式应用提供高效且可靠的**分布式协调服务**

**CP（一致性+分区容忍性）**

基于**<strong style="color:red">观察者模式</strong>**设计的分布式服务管理框架。负责存储和管理数据，然后接受观察者的注册，一旦数据状态发生变化，zookeeper将通知注册的观察者做出相应的反应，从而实现集群中类似的Maset/Slaver管理模式

**<strong style="color:red">文件系统（基于层次型的目录树的数据结构）+通知机制</strong>**

## 5.1 应用场景

> 其实都是将对应的数据（如配置、客户端ip信息）挂载在一个指定的Znode上，通过监听Znode子节点的变化（或数据、或节点）去动态实时响应。

1.  集群统一配置管理

2. 集群统一命名服务：命名服务是通过对资源命名，然后通过命名去定位资源

3. 集群统一管理

   如：服务上下线动态感知，节点运行状态

   **临时节点+监听机制**

4. 负载均衡

5. 分布式消息同步和协调机制

6. 对Dubbo的支持

7. [分布式锁](#_3.zookeeper实现)

   <strong style="color:red">有序临时节点 + 监听机制</strong>

   3个请求同时发送到3个服务器上操作同一共享资源，
    A请求 ----- server1 B请求-----server2 C请求----server3

   每个请求都会在zookeeper下的lock节点下生成一个临时有序节点：

   ​    A：001 B:002 C:003

   在操作资源之前，

   1.先检查自身id是否是lock子节点中最小值，

   2.如果是，则获取锁，进行操作，操作完成后释放锁（即删除自身节点）。

   3.如果不是，添加上一个节点的监听事件，等待上一个节点的删除事件。

## 5.2 特性

1. 顺序一致性

   所有更新都有一个全局有序的zxid（zookeeper transaction id）

2. 原子性

3. 单一视图

4. 可靠性

5. 实时性（最终一致性）

## 5.3 常用命令

```shell
# 进入服务器 
docker exec -it zk3 bash
# 查看节点的状态
./bin/zkServer.sh status
# 开启客户端
zkCli.sh
# 创建临时节点，当客户端关闭时候，该节点会随之删除。不加参数－e创建永久节点。
create -e /node1 node1.1 
# 获取节点值
get /node
# 列出节点
ls /node
# 删除节点
delete /node 
# 查看节点信息
stat
# 用于设置节点访问权限
setAct path acl
# 查看节点的权限信息
getAcl path
```

## 5.4 内部原理

### 5.4.1 三种角色

- **Leader**

- **Follower**

- **Observer**

  可以理解为不参与投票的Follower，协助Follower处理读请求。当集群中读请求负载很高时，为什么不增加Follower节点，原因是写请求需要Follower节点超过半数同意，会增加Leader和Follower的通信压力，降低写效率。

### 5.4.2 两种模式

- 恢复模式

  当服务启动或Leader崩溃后，zk进入恢复状态，选举leader，选举完成后将leader与其他节点数据同步，当大多数Follower与leader同步完成后，恢复模式完成。

- 广播模式

  在恢复模式完成后，客户端发起写请求，leader采用ZAB协议广播该写请求，超过半数的follower同意后提交该事务，完成本次请求。follower要么ack，要么放弃，leader无需等待所有的Follower应答。

### 5.4.3 zxid

​	zxid Long(64) 纪元epoch（高32位）+ xid（低32位）

​	epoch：每个leader都有自己的纪元

​	xid：依次递增的事务id

 使用方式：

 	在广播模式中，leader会生成一个zxid和写请求一并发送给Follower，follower本地也有自己的zxid，如果leader的zxid > follower，follower将leader.zxid写入本地日志中，返回ack应答，否则拒绝响应。

### 5.4.4 选举机制

​	**Server ID**： myid(权重越大)
​	**Zxid**：数据ID(先一数据低进行选择)

​	**半数机制（**[**ZAB**](#_六、ZAB协议)**）：**

​	集群中半数以上机器存活，集群可用。所以zookeeper适合装在奇数台机器上。

​	Leader是通过内部的选举机制临时产生的。

​	以一个简单的例子来说明整个选举的过程。

​	假设有五台服务器组成的zookeeper集群，它们的myid从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。

​	(1)服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。

​	(2)服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。

​	(3)服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。

​	(4)服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。

​	(5)服务器5启动，同4一样当小弟。

## 5.5 Znode节点类型

Znode = path + nodeValue + Stat

![image-20210607093411083](E:\workspace\github\book-mark\images\zookeeper05.png)



## 5.6 zoo.cfg

(1) tickTime 通信心跳时间 session超时时间是两倍tickTime

(2) initLimit LF初始通信时限 Leader与Follower建立连接的时间

(3) syncLimit LF同步通信时限 Leader与Follower之间的最大响应时间

(4) dataDir 数据存储目录

(5) clientPort 客户端连接zookeeper端口，默认是2181

## 5.7 Stat结构体

![image-20210607093609599](E:\workspace\github\book-mark\images\zookeeper06.png)

## 5.8 监听器原理

一次触发：数据发生改变后，watch只会监听一次，当数据再次改变时，需要创建新的watch去监听此次改变事件

数据观察和子节点观察

(1) 首先要有一个main()线程

(2) 在main线程中创建ZK客户端，这是会创建两个线程，一个负责网络连接通信(connect),一个负责监听(listener)

(3) 通过connect线程将注册的监听事件发送给ZK

(4) 在ZK的注册监听器列表中将注册的监听事件添加到列表中

(5) ZK监听到有数据或路径发生变化时，就会将这个消息发送给listener线程

(6) Listener线程内部调用process()方法

![image-20210607093700334](E:\workspace\github\book-mark\images\zookeeper07.png)
