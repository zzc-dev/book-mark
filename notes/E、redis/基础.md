# 一、特性

1. 非关系型的键值对数据库，可以根据键以O(1)的时间复杂度查找或插入关联值

   > 整个key-value是hash表存储

2. Redis的数据是存在内存中的，性能高

3. 键的类型是唯一的，可以是字符串、整形、浮点型

4. 值的类似可以是：string、list、set、sorted set、hash

5. Redis内置了复制、磁盘持久化、LUA脚本、事务、客户端代理等功能

6. 通过哨兵和自动分区提供高可用性

7. 单线程每秒数十万级别的处理能力

## 1. Rehash

​	redis的hashtable扩容时，为了避免阻塞客户端请求，采用渐进式rehash，并且有两个hashtable，

​	查询时先查新的，查不到在查旧的；插入在新的hash表，更新删除新旧都操作

​	**Scan查询时发生Rehash**：

​		<strong style="color:red">Rehash采用高位进位法，Scan遍历key时，rehash扩容不会漏key，再缩容时会有重复key</strong>

​		https://blog.csdn.net/u014439693/article/details/108325632

## 2. 高性能的IO模型--多路复用模型

>Redis单线程：主要是指网络IO和键值对读写是一个线程完成的。
>                   持久化、异步删除、集群数据同步是由额外的线程执行的。  

多线程问题：共享资源的并发访问+线程的上下文切换

**Redis单线程快的原因：**

​	<strong style="color:red">内存读写+高效的数据结构+多路复用IO模型</strong>

**多路复用IO模型：**

​    并发处理大量的客户端请求，提升吞吐量

### 2.1 单线程处理网络IO的问题

<img src="E:\workspace\github\book-mark\images\redis10.jpg" alt="img" style="zoom: 25%;" />



阻塞点：

	1. accept：Redis监听到一个连接请求，但一直未能成功建立连接
 	2. recv：Redis通过recv从一个客户端读取数据，如果数据一直无法到达

   以上两点都会导致Redis线程阻塞，无法处理其他客户端请求

 解决：使用Socket网络模型：支持非阻塞模式

### 2.2 非阻塞模式

在 socket 模型中，不同操作调用后会返回不同的套接字类型。socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字

<img src="E:\workspace\github\book-mark\images\redis11.jpg" alt="img" style="zoom: 25%;" />

调用accept和recv方法时，使用监听套接字监听，当连接请求或数据未到达时，Redis线程可以去处理其他事情，无需等待
当数据到达时，通过某种机制去通知Redis处理，这时，我们可以用到Linux的IO多路复用机制

### 2.3 基于多路复用的高性能I/O模型

Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制

在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字

<img src="E:\workspace\github\book-mark\images\redis12.jpg" alt="img" style="zoom:25%;" />

Redis调用epoll，让内核监听这些套接字（FD）,一旦套接字上有请求达到时，根据事件调用相应的回调函数

## 3. 持久化

原因：数据存在内存中，一旦服务器宕机，内存数据将全部丢失

为什么不从数据库中同步：

1. 需要频繁访问数据库，给数据库带来巨大的压力

 	2. 读取数据库性能低

### 3.1 AOF

记录的是命令而不是数据

例如: 执行 set testkey testvalue，保存的是：

```
*3         当前命令有三个部分
$3         命令有几个字节
set
$7
testkey
$9
testvalue
```

#### 3.1.1 写回策略

1. **always**：每次写操作同步写入磁盘

 	2. **EverySec**：每秒写回，每个写命令执行完，先吧日志写到AOF的内存缓冲区，每隔1秒把日志写入磁盘
 	3. **NO**：操作系统写回，每个写命令执行完，先吧日志写到AOF的内存缓冲区，由操作系统决定何时写入磁盘

主线程阻塞和数据丢失风险，无法做到两全其美

<img src="E:\workspace\github\book-mark\images\redis13.jpg" alt="img" style="zoom:33%;" />

<img src="E:\workspace\github\book-mark\images\redis21.jpg" alt="img" style="zoom:33%;" />

当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢

随着命令的执行，AOF日志文件会越来越大，性能也会受到影响：

1. 磁盘文件的大小本身收到限制
 	2. 文件过大，每次写入性能降低
 	3. 服务宕机，恢复过程缓慢

**是否配置EverySec就不会阻塞主线程？**

​	当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢。

​	解决：如果业务应用对延迟非常敏感，但同时允许一定量的数据丢失，作如下的配置

```shell
# 在 AOF 重写时，不进行 fsync 操作
no-appendfsync-on-rewrite yes
```

#### 3.1.2 AOF日志重写机制

> AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入

​    <strong style="color:red">AOF重写会阻塞主线程吗</strong>

​    和AOF文件是由主线程写入不同，重写过程是后台子进程`berewriteaof`完成的。

**一个拷贝，两处日志**

“一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。

“两处日志”：因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 **AOF 日志**，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复

而第二处日志，就是指新的 **AOF 重写日志**。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。

​     <strong style="color:red">AOF fork子进程风险</strong>

1. fork这个瞬间一定是阻塞主线程的，fork不会一次性拷贝所有内存数据给子进程，而是采用操作系统提供的**写时复制（Copy-On-Write**）就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞

   但fork子进程需要拷贝进程必要的数据结构，其中有一项就是**拷贝内存页表**（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，**实例越大，内存页表越大，fork阻塞时间越久**。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。

2. fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。

   此时主线程仍然有数据写入，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。

   因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个**bigkey**，**重新申请大块内存耗时会变长**，可能会产阻塞风险。

   如果操作系统开启了**内存大页机制**(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。

   Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。

#### 3.1.3 如何触发AOF重写

 1. 手动执行命令：`bgrewriteaof`

 2. 通过配置文件自动触发

    ```
    1. auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认为64MB
    2. auto-aof-rewrite-percentage: 这个值的计算方法是：当前AOF文件大小和上一次重写后AOF文件大小的差值，再除以上一次重写后AOF文件大小。也就是当前AOF文件比上一次重写后AOF文件的增量大小，和上一次重写后AOF文件大小的比值。
    
    AOF文件大小同时超出上面这两个配置项时，会触发AOF重写。
    ```


**优缺点：**

   优点：每次操作只需记录命令，需要持久化的数据量不大

   缺点：进行故障恢复时，需要逐一把操作执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。

### 3.2 RDB

> **内存快照**：记录的是内存某一时刻的数据，在做故障恢复的时候，直接把数据读入内存即可，

<strong style="color:red">做快照时，数据还能被增删改吗？(主线程会被阻塞吗)</strong>

   Redis提供两个命令来生成RDB文件：

		1. **save**：在主线程中执行，会导致阻塞；
  		2. **bgsave**：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。

使用bgsave生成rdb文件时，采用了**写时复制**，避免阻塞写请求

>与AOF一样，fork拷贝内存页表，主线程读操作不受影响，当有写操作时，并且key存在快照中时，主线程拷贝一份数据到新的内存，并把这个内存地址置为主线程的虚拟内存地址执向它，这样，父子进程内存真正分离，主线程的写请求也不会受到影响。

**快照生成的频率**

​	频繁地执行全量快照，也会带来两方面的开销。

    1. 频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环
       2. bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了

**增量快照：**

> **Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照**

**如何触发RDB**

1. shutdown(正常关闭)时，如果没有开启aof，会触发；

2. redis.conf默认配置(此配置是针对bgsave)

   根据这个默认配置，会丢数据；意外宕机的情况下，丢失最后一次持久化后的数据

   ```mysql
   save 900 1  #900s检查一次，增量的数据变更命令超过1，就触发；
   save 300 10 #300s检查一次 更改10次
   sava 60 10000 #60s检查一次  更改命令1w条，就触发；
   ```

3. 执行命令save或者bgsave

4. 执行**flushall命令**，清空rdb(Redis默认16个库都清空)；但是里面是空的，无意义

**关于 AOF 和 RDB 的选择问题的建议**：

1. 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择；
2. 如果允许分钟级别的数据丢失，可以只使用 RDB；
3. 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡

## 4. 主从同步

如果 Redis 发生了宕机，它们可以分别通过回放日志和重新读入 RDB 文件的方式恢复数据，从而保证尽量**少丢失数据**，提升可靠性

> Redis高可靠性：
>
> 1. 数据尽量少丢失： AOF/RDB
>
> 2. 服务尽量少中断： 主从复制：数据保存到多个实例中

Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式

​	读操作：主库、从库都可以接收；
​	写操作：首先到主库执行，然后，主库将写操作同步给从库。

### 4.1 主从库第一次全量同步 

```C
replicaof 主库ip 主库端口
```

<img src="E:\workspace\github\book-mark\images\redis14.jpg" alt="img" style="zoom:50%;" />

**1. 主从库间建立连接、协商同步的过程，主要是为全量复制做准备**

从库发送psync ？ -1 命令，主库根据命令启动复制

```
psync 主库runId 复制进度offset
runId 是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”
offset 此时设为 -1，表示第一次复制
```

主库收到 psync 命令后，会用 `FULLRESYNC` 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数

`FULLRESYNC` 响应表示第一次复制采用的**全量复制**

**2. 主库发送所有数据给从库**

​	  主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件

  	<strong style="color:red">在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求</strong>

​      为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作

**3. 主库将新接收的写命令同步给从库**

​	  当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了

### 4.2 主从级联模式分担主库全量复制时的压力

我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系

```
replicaof 所选从库的IP 6379
```

<img src="E:\workspace\github\book-mark\images\redis15.jpg" alt="img" style="zoom:50%;" />



### 4.3 网络断链导致数据不一致

如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法和主库保持一致了，客户端就可能从从库读到旧数据

从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步

> **replication buffer**  每个客户端有一个
> 	client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的
> 	Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互
>     主从库断开连接后，这个client buffer也消失了
>
> **repl_backlog_buffer** 环形 共享资源
>
> 它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer

主从库增量复制：
	主库接收到写命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区

  ![img](E:\workspace\github\book-mark\images\redis16.jpg)

master_repl_offset： 主库写的偏移量
slave_repl_offset： 从库读的偏移量，从库会保存

从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等

主从库断开连接后，新的写入命令写入repl_backlog_buffer 

主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距

**如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致**

因此，我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值

### 4.4 建议

​	全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的。

​	建议：一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力

### 4.5 问题： 主从库间的复制不使用 AOF 呢？

​	RDB文件是压缩的二进制数据，文件小，减小文件传输机器的网络带宽
​                                                                        从库读取速度快。从库按照RDB协议解析还原速度快			

​			  打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能，而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的

### 4.6 主从同步总结

**全量同步+增量同步**

1.  主从建立链接 `replicaof 主库ip port`，

    主库为每个从库创建了一个`repl buffer`

    主库自身有1个环形缓冲区`repl_backlog_buffer`

2.  从机发送 psync  runID offset  `psync ? -1` 

3.  主机接收到从机的同步请求，offset=-1，全量同步，`bgsave`生成RDB文件，fullresync runID offset，把主机runID和主机的复制进度offset给从库，从库保  存这些信息

4.  主库发送RDB文件给从库

​    主从同步期间，主库可以处理写请求，然后把写命令缓存到`repl buffer`中，当主库发送RDB文件后，就把`repl buffer`中的写命令发给从库，此后的写命令都是放入`repl buffer`中，然后同步给从库，实现增量更新

**从库断开链接的同步**

​	主库的写命令都会写入`repl buffer`和`repl_backlog_buffer`中

1. 从库断开链接，主库中该从库的`rep_buffer`被移除了：此时从库保存这主库的runID、从库已复制偏移量`slave_repl_offset`

2. 从库重新建立链接后，`rep_buffer`又被创建，`psync runID slave_repl_offset`
3. 主库去`repl_backlog_buffer`找从库的offset，
   - 如果找到，把（slave_repl_offset,master_repl_offset]的写命令发送到`repl_buffer`，然后同步给从库
   - 没有找到，则全量同步

### 4.7 主从同步问题

 <strong style="color:blue" >数据不一致</strong>

​	原因：

			1. 网络延迟
   			2. 从库正在执行耗时的命令导致无法处理主库的同步命令

   解决方案：

	1. 在硬件环境配置方面，我们要尽量保证主从库间的网络连接状况良好
 	2. 开发外部程序监控主从复制差异进度。大于一定阈值，不允许从库可读

<strong style="color:blue" >读取过期数据</strong>

- 定期删除key（随机选取一定量的数据），会导致没有被选取的过期key被读取
- 惰性删除
  - 主库读取到过期key删除返回空，但从库没有读取，不会执行删除操作，下次读取从库被读取
  - 从库读取过期key不会删除
    - Redis3.2前，返回key
    - Redis3.2及之后，虽然不会删除，但会返回null	

- 过期命令使用不当
  - EXPIRE 和 PEXPIRE：它们给数据设置的是从命令执行时开始计算的存活时间；
  - EXPIREAT 和 PEXPIREAT：它们会直接把数据的过期时间设置为具体的一个时间点

<strong style="color:blue" >不正确的配置项导致服务挂掉</strong>

​	`protected-mode` 限定哨兵实例能否被其他服务器访问

​		yes: 只能本地访问； no：其他服务器也可访问

```shell
# 只有本地和bind中的ip可以访问该哨兵
protected-mode no
bind 192.168.10.3 192.168.10.4 192.168.10.5
```

​	`cluster-node-timeout`实例响应心跳消息的超时时间

### 4.8 脑裂

主库故障造成数据丢失

	1. 主库处理写命令时还未同步给从库，主库故障，选取新主库，丢失数据
 	2. 脑裂：主库假故障

脑裂

1. 主库假故障，无法响应哨兵，哨兵集群开始选举新主库
2. 原主库故障解除，客户端发送写命令
3. 选主完成，哨兵就会让原主库执行 slave of 命令，原主库清空本地数据，造成数据丢失

<img src="E:\workspace\github\book-mark\images\redis31.jpg" alt="img" style="zoom: 25%;" />

解决：主库假故障时无法接受客户端的请求

```shell
min-slaves-to-write #主库能进行数据同步的最少从库数量
min-slaves-max-lag  #主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）

举例：
	min-slaves-to-write=1
	min-slaves-max-lag=12s，
	down-after-milliseconds=10s
	主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。
	同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。
	这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了
```

## 5. 哨兵机制

主库挂掉切换新主库三问：

1. 主库真的挂了吗？
2. 该选择哪个从库作为主库？
3. 怎么把新主库的相关信息通知给从库和客户端呢？

哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。

### 5.1 基本流程

哨兵主要负责的就是三个任务：**监控、选主（选择主库）和通知**。

#### 5.1.1 监控

哨兵周期性地向主库、从库发送ping，如果不能及时响应则，判定下线

<img src="E:\workspace\github\book-mark\images\redis17.jpg" alt="img" style="zoom:33%;" />



- **主观下线**：

  **哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态**

  从库超时：直接标记为下线
  ​主库超时：直接标记为下线，可能存在哨兵误判，切换主库过程开销大，需要减少误判
  ​				   误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下

- **客观下线：**

  ​	使用哨兵集群：少数服从多数的原则判断主库是否断开连接
  ​    只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”

### 5.2 如何选择新主库

**筛选+打分**

- 筛选：在选主时，**除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**

  配置项 `down-after-milliseconds * 10`。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。

> **要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds**。我们曾经就踩过一个“坑”。当时，在我们的项目中，因为这个值在不同的哨兵实例上配置不一致，导致哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定

- 打分
  -  优先级最高的从库得分高  配置：`slave-priority`
  - 和旧主库同步程度最接近的从库得分高
  - ID 号小的从库得分高

### 5.3 配置

```shell
down-after-milliseconds #主从库断连的最大链接超时时间（主观下线时间）
slave-priority # 从库优先级 内存大的优先级可以高些
```

### 5.4 问题

1. **主库下线了，如何判断从库的同步程度**

   master_repl_offset和slave_repl_offset都是单调递增的，只需判断大小就行

2. **哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作**

   **读写分离模式：读可以（从库支持），写不行（主库尚未选举出来）**

3. **`down-after-milliseconds`** 是不是配置的越大越好？

   不是。可以适当配置大些，可以减少哨兵误判的概率。

   |                              | 优势                                               | 劣势                                                     |
   | ---------------------------- | -------------------------------------------------- | -------------------------------------------------------- |
   | 配置的时间越短，哨兵越敏感   | 当主库真正故障时，因为切换得及时，对业务的影响最小 | 误判几率大，可能因为网络拥塞但主库正常而发生不必要的切换 |
   | 配置的时间比较长，哨兵越保守 | 减少哨兵误判的概率                                 | 主库故障发生时，业务写失败的时间也会比较久               |

4. **应用程序如何感知主从切换已完成，主库地址发生改变？**

   - **哨兵主动通知客户端**。客户端订阅`switch-master`，当主库地址改变时，哨兵会把主库地址发布到`switch-master`
   - **客户端主动获取**。客户端不能写死主库地址，每次发送写命令时主动从哨兵集群获取最新地址：`sentinel get-master-addr-by-name`

5. **哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？**

   存在故障节点时，只要集群中大多数节点状态正常，集群依旧可以对外提供服务

## 6. 哨兵集群

```
# 配置哨兵 设置主库的 IP 和端口，并没有配置其他哨兵的连接信息
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
```

### 6.1 基于pub/sub机制的哨兵集群组成

哨兵之间建立连接：订阅主库的`sentinel:hello`，并在该频道发布自己的信息

哨兵与从机建立连接：向主库发送 `info` 命令，得到从库列表

### 6.2 基于 pub/sub 机制的客户端事件通知

> 从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。

所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件

<img src="E:\workspace\github\book-mark\images\redis18.jpg" alt="img" style="zoom:33%;" />

### 6.3 由哪个哨兵执行主从切换？

**客观下线的具体流程：**

​	任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。

​	客观下线的条件：一个哨兵获取仲裁的赞成票 >= 该哨兵配置 `quorum`

​	判断主库客观下线后，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。

**Leader 选举**

​	任何一个想成为 Leader 的哨兵，要满足两个条件：

​		第一，拿到半数以上的赞成票；

​		第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

<img src="E:\workspace\github\book-mark\images\redis19.jpg" alt="img" style="zoom:33%;" />

如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍），再重新选举。这是因为，哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。

###  6.4 Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换

1、哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”

2、但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。

### 6.5 哨兵实例是不是越多越好？

我们也看到了，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。

## 7. 切片集群

随着数据量的增长，Redis的响应越来越慢，这和Redis 的持久化机制有关系。在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了

**纵向扩展**：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。
			纵向扩展会受到硬件和成本的限制

**横向扩展**：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。

### 7.1 数据切片和实例的对应分布关系

​	Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。

​	每个Hash Slot对应着一个实例。

​	存key：hash(key)/16384 --> hashslot --> 实例

​	获取key：hash(key)/16384 --> hashslot --> 实例

- <strong style="color:blue">自动分配</strong>：使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上

- <strong style="color:blue">手动分配</strong>：cluster addslots

  **在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。**

### 7.2 客户端如何定位数据

**每个实例拥有所有实例的hash slot信息**，Redis实例会把自己的hash slot发送给和它相连接的实例。

客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端

客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了

在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：

- 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽；
- 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍

Redis Cluster 方案提供了一种重定向机制，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。

```
GET hello:key
(error) MOVED 13320 172.16.19.5:6379
```

当hash slot已经迁移后，客户端读取key时，返回moved 和该key新的hash slot对应的实例，客户端根据地址再次发送即可

但如果此时hash slot正在迁移，如key1和key2已经迁移，但key3还没有，此时客户端访问key2，返回ACK
客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据

```
GET key2
(error) ASK 13320 172.16.19.5:6379
```

### 7.3 Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：

1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。

2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。

3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。

4、而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。

5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。

### 7.4 数据倾斜

- **数据量倾斜**：在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多。
- **数据访问倾斜**：虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点数据，被访问得非常频繁。

#### 7.4.1 **数据量倾斜**

<strong style="color:blue">成因：</strong>

- bigkey导致倾斜
- Slot 分配不均衡导致倾斜
- Hash Tag 导致倾斜

>HashTag 指加在键值对 key 中的一对花括号{}。这对括号会把 key 的一部分括起来，客户端在计算 key 的 CRC16 值时，只对 Hash Tag 花括号中的 key 内容进行计算
>
>它主要是用在 Redis Cluster 和 Codis 中，支持事务操作和范围查询。因为 Redis Cluster 和 Codis 本身并不支持跨实例的事务操作和范围查询，当业务应用有这些需求时，就只能先把这些数据读取到业务层进行事务处理，或者是逐个查询每个实例，得到范围查询的结果

#### 7.4.2 数据访问倾斜

**发生数据访问倾斜的根本原因，就是实例上存在热点数据**

热点数据多副本方法只能针对只读的热点数据

## 8. 缓冲区

> 一块内存空间来暂时存放命令数据，以免出现因为数据和命令的处理速度慢于发送速度而导致的数据丢失和性能问题

### 8.1 客户端输入输出缓冲区

<img src="E:\workspace\github\book-mark\images\redis25.jpg" alt="img" style="zoom:25%;" />

#### 8.1.1 输入缓冲区溢出

	1. 写入bigkey
 	2. 服务器端处理请求的速度过慢

查看输入缓冲区的使用情况

```shell
CLIENT LIST
返回
id=5 addr=127.0.0.1:50487 fd=9 name= age=4 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client

cmd，表示客户端最新执行的命令。
qbuf，表示输入缓冲区已经使用的大小。
qbuf-free，表示输入缓冲区尚未使用的大小
```

解决：

 1. **把缓冲区调大**

    Redis写死了，Redis 服务器端允许为每个客户端最多暂存 1GB 的命令和数据

 2. **从数据命令的发送和处理速度入手**。

    避免客户端写入 bigkey，以及避免 Redis 主线程阻塞

#### 8.1.2 输出缓冲区溢出

输出缓冲区包括两部分：

		1. 一个大小为 16KB 的固定缓冲空间，用来暂存 OK 响应和出错信息；
  		2. 一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。

发生溢出：

	1. 服务器端返回 bigkey 的大量结果；
 	2. 执行了 MONITOR 命令 （生产和环境禁用）
 	3. 缓冲区大小设置得不合理。

设置输出缓冲区大小

```shell
# 1.设置缓冲区大小的上限阈值；
# 2.设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值
config set client-output-buffer-limit

举例：
# 1.普通客户端（不做限制）
client-output-buffer-limit normal          0              0             0
                           普通客户端  缓冲区大小限制  缓冲区持续写入量限制 持续写入时间限制
# 2.订阅客户端：关闭客户端的连接情况
#   1.超过8m，
#   2.60s内写入超过2M 
client-output-buffer-limit pubsub         8mb             2mb           60

# 3. 从库repl_buffer大小
client-output-buffer-limit slave         512mb            128mb         60   
```

### 8.2 主从集群缓冲区

全量复制：为每个客户端分配了一个 `repl_buffer`，当从库接收和加载RDB较慢时，可能会导致缓冲区溢出，主库会断开和从库的连接。

​	解决：1. 主节点保存的数据了大小控制在2~4GB，使全量同步执行快些

​			    2. 合理设置缓冲区大小 `client-output-buffer-limi`

​				3. 控制从库的个数

增量复制（断链重连后的复制）:  `repl_backlog_buffer` 环形缓冲区，一旦写满，会覆盖命令，造成全量复制

​		可以设置大小：`repl_backlog_size`

## 9. 原子操作

- 单命令操作
  - incr/decr  增值/减值
  - setnx key value 如果key不存在，保存成功
- 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本

## 10. 事务

```shell
MULTI  #开启事务

EXEC   #提交事务
```

RDB 快照不会在事务执行时执行

Redis官方事务地址：https://redis.io/topics/transactions

### 10.1 ACID

- 原子性

  - 命令语法问题，在入队检测出错，EXEC提交时拒绝执行事务。保证原子性
  - 事务操作入队Redis无法检测出错，但在执行命令时报错，Redis还是会把正确的命令执行完。不保证原子性
  - 执行EXTC时，Redis故障发生。如果开启了AOF日志，使用redis-check-aof 工具检查 AOF 日志文件，这个工具可以把未完成的事务操作从 AOF 文件中去除。保证原子性

- 一致性

  保证一致性。（一致性概念没理解）

- 隔离性

  - EXEC执行前，需要使用WATCH机制保证隔离性

    WATCH机制：监听一个key或多个key，看事务提交前是否被修改，如果被修改，在EXEC时放弃执行事务

    <img src="E:\workspace\github\book-mark\images\redis30.jpg" alt="img" style="zoom:25%;" />

  - EXEC执行后，Redis单线程，保证隔离性

- 持久性

  AOF和RDB都有丢失数据的风险，不能保证持久性

### 10.2 建议

 1. 使用事务时，建议和Pipeline一起使用

    一次性将所有命令打包给Redis，不仅减少了网络IO，并且Redis是单线程的，处理过程不会被别的请求打断，保证了隔离性

 2. WATCH命令的使用场景

    对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的原子性，此时就需要用到 WATCH 了。例如想要修改某个资源，但需要事先读取它的值，再基于这个值进行计算后写回，如果在这期间担心这个资源被其他客户端修改了，那么可以先 WATCH 这个资源，再读取、修改、写回，如果写回成功，说明其他客户端在这期间没有修改这个资源。如果其他客户端修改了这个资源，那么这个事务操作会返回失败，不会执行，从而保证了原子性。

## 11. 过期key删除策略

### 11.1 惰性删除

> 当一个数据的过期时间到了以后，并不会立即删除数据，而是等到再有请求来读写这个数据时，对数据进行检查，如果发现数据已经过期了，再删除这个数据

优点：

​	尽量减少删除操作对 CPU 资源的使用，对于用不到的数据，就不再浪费时间进行检查和删除了。

缺点：

​	导致大量已经过期的数据留存在内存中，占用较多的内存资源

### 11.2 定期删除

> Redis 每隔一段时间（默认 100ms），就会随机选出一定数量的数据，检查它们是否过期，并把其中过期的数据删除，这样就可以及时释放一些内存

# 二、统计选取数据类型

## 2.1 聚合统计

当需要对多个集合做聚合统计时，推荐使用set。

> 所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括：统计多个集合的共有元素（交集统计）；把两个集合相比，统计其中一个集合独有的元素（差集统计）；统计多个集合的所有元素（并集统计）

```
# user:id和user:id:0207的并集结果存到user:id
SUNIONSTORE user:id user:id user:id:0207 

# user:id和user:id:0207的差集结果存到user:new
SDIFFSTORE user:new user:id user:id:0207

# user:id和user:id:0207的交集结果存到user:id:rem
SINTERSTORE user:id:rem user:id user:id:0207
```

当集合比较大时，聚合操作将阻塞主线程。

Set数据类型，使用SUNIONSTORE、SDIFFSTORE、SINTERSTORE做并集、差集、交集时，选择一个从库进行聚合计算”。这3个命令都会在Redis中生成一个新key，而从库默认是readonly不可写的，所以这些命令只能在主库使用。想在从库上操作，可以使用**SUNION、SDIFF、SINTER**，这些命令可以计算出结果，但**不会生成新key**

改进：

1. 指定一个从库做复杂的聚合运算

 	2. 读取数据到客户端进行聚合运算

**注意**

​		如果是在集群模式使用多个key聚合计算的命令，一定要注意，因为这些key可能分布在不同的实例上，**多个实例之间是无法做聚合运算的**，这样操作可能会直接报错或者得到的结果是错误的！

## 2.2 排序统计

面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set

## 2.3 二值统计

bitmap

## 2.4 基数统计

> 基数统计就是指统计一个集合中不重复的元素个数

set、hashtable随着集合元素的增多，占用的空间也越来越大

`HyperLogLog`**是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小**

在 Redis 中，每个 HyperLogLog 只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数。你看，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间

在统计 UV 时，你可以用 PFADD 命令（用于向 HyperLogLog 中添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中。

```
PFADD page1:uv user1 user2 user3 user4 user5

#返回统计结果
PFCOUNT page1:uv
```

**HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。**

如果集合元素量达到亿级别而且不需要精确统计时，我建议你使用 HyperLogLog

## 2.5 总结

当需要进行排序统计时，List 中的元素虽然有序，但是一旦有新元素插入，原来的元素在 List 中的位置就会移动，那么，按位置读取的排序结果可能就不准确了。而 Sorted Set 本身是按照集合元素的权重排序，可以准确地按序获取结果，所以建议你优先使用它

<img src="E:\workspace\github\book-mark\images\redis20.png" alt="img" style="zoom: 25%;" />

# 三、操作系统内存机制

## 3.1 Swap

> **操作系统的内存 swap**: 操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，涉及到磁盘的读写

swap 触发后影响的是 Redis 主 IO 线程，这会极大地增加 Redis 的响应时间

```
##获取redis的进程号
$ redis-cli info | grep process_id
process_id: 5332

#进入 Redis 所在机器的 /proc 目录下的该进程目录中
$ cd /proc/5332

#查看该 Redis 进程的使用情况
# Size：内存占用大小， Swap：该内存区域有多少被交换到磁盘  Size==Swap 内存已经完全被交换到磁盘了
$cat smaps | egrep '^(Swap|Size)'
Size: 584 kB
Swap: 0 kB
Size: 4 kB
Swap: 4 kB
Size: 4 kB
Swap: 0 kB
Size: 462044 kB
Swap: 462008 kB
Size: 21392 kB
Swap: 0 kB
```

增大机器内存，将其他操作大量文件的程序迁移出去

## 3.2 内存大页机制

该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的

   虽然内存大页可以给 Redis 带来内存分配方面的收益，但是，不要忘了，Redis 为了提供数据可靠性保证，需要将数据做**持久化保存**。
这个写入过程由额外的线程执行，所以，此时，Redis 主线程仍然可以接收客户端写请求。客户端的写请求可能会修改正在进行持久化的数据。在这一过程中，Redis 就会采用**写时复制机制**，也就是说，一旦有数据要被修改，Redis 并不会直接修改内存中的数据，而是将这些数据拷贝一份，然后再进行修改。

如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝 2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB。两者相比，你可以看到，当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，这就会影响 Redis 正常的访存操作，最终导致性能变慢。

```
# always：启动 no：未启动
cat /sys/kernel/mm/transparent_hugepage/enabled

#关闭内存大页机制
echo never /sys/kernel/mm/transparent_hugepage/enabled
```

## 3.3 内存分配

**当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会立即返回给操作系统。所以，操作系统仍然会记录着给 Redis 分配了大量内存**

> 内存碎片：
>
> ​	内存碎片的形成有内因和外因两个层面的原因。简单来说，内因是操作系统的内存分配机制，外因是 Redis 的负载特征。

**内存分配器的分配策略**

​	Redis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，默认使用 `jemalloc`

​    **Jemalloc** 按照一系列固定大小划分内存空间

​		例如 8 字节、16 字节、32 字节、48 字节，…, 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间

**外因：键值对大小不一样和删改操作**

   根据jemalloc分配的是一个固定大小的内存空间，修改和删除这些键值对，会导致空间的扩容和释放。

   举例：

<img src="E:\workspace\github\book-mark\images\redis22.jpg" alt="img" style="zoom: 25%;" />

1. A、B、C、D分别保存3、1、2、4字节
2. D删除1byte
3. A修改了数据，占用4个字节，为了保证A数据的空间连续性，操作系统需要把B拷贝到别的空间，比如：D删除的1byte空间
4. C和D分别删除2byte和1byte
5. 应用E申请一个3byte的连续的内存空间，虽然有3个空闲的内存空间，但是不连续，这就形成了内存碎片

```

INFO memory
# Memory
used_memory:1073741736 #redis实际申请使用的空间
used_memory_human:1024.00M
used_memory_rss:1997159792 # 操作系统实际分配的内存空间大小包括碎片
used_memory_rss_human:1.86G
…
mem_fragmentation_ratio:1.86 #内存碎片率

mem_fragmentation_ratio = used_memory_rss/ used_memory
```

**如何清理内存碎片？**

1. 重启 Redis 实例
2. 从 4.0-RC3 版本以后，Redis 自身提供了一种内存碎片自动清理的方法

​     **碎片清理是有代价的**，操作系统需要把多份数据拷贝到新位置，把原有空间释放出来，这会带来时间开销。因为 Redis 是单线程，在数据拷贝时，Redis 只能等着，这就导致 Redis 无法及时处理请求，性能就会降低

    启用自动内存碎片清理
    config set activedefrag yes
    
    下面两个条件都满足时开启自动清理
    1.内存碎片的字节数达到 100MB 
    active-defrag-ignore-bytes 100mb
    2.内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10%
    active-defrag-threshold-lower 10
为了尽可能减少碎片清理对 Redis 正常请求处理的影响，自动内存碎片清理功能在执行时，还会监控清理操作占用的 CPU 时间，而且还设置了两个参数，分别用于控制清理操作占用的 CPU 时间比例的上、下限，既保证清理工作能正常进行，又避免了降低 Redis 性能

```
# 自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展
active-defrag-cycle-min 25
# 自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。
active-defrag-cycle-max 75：
```

# 四、阻塞操作

<img src="E:\workspace\github\book-mark\images\redis24.jpg" alt="img" style="zoom: 25%;" />

1. 操作复杂度高的命令。如：集合全量查询和聚合操作

2. 分配或释放大量内存。操作bigkey，清空数据库flushall  ----可以开启子线程执行

   **释放内存操作系统会维护一张空闲内存块的链表**。释放内存越大，链表操作时间越长，阻塞主线程

3. 同步写入AOF ----可以开启子线程执行

4. RDB文件太大，fork子线程时复制内存映射表花费时间长

5. 主从同步

   1. 从库需清空库 -- 可以开启子线程执行
   2. 读取RDB文件
   
6.  大量过期key同一时间过期

## 5.1 异步删除

​	主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为**惰性删除**（lazy free）。此时，删除或清空操作不会阻塞主线程，这就避免了对主线程的性能影响。

## 5.2 AOF阻塞详解

写回策略配置：

`everySec`，和**惰性删除**类似，主线程把命令封装为任务加入任务队列中，子线程每秒去队列中读取任务写入AOF日志文件

`alaways` 默认主线程执行，也可以fork子进程读取任务队列

## 5.3 lazy free（了解）

​	lazy-free是4.0新增的功能，但是默认是关闭的，需要手动开启。

​	即使开启了lazy-free，如果直接使用DEL命令还是会同步删除key，只有使用UNLINK命令才会可能异步删除key。

​			FLUSHDB ASYNC
​			FLUSHALL AYSNC
​            UNLINK

​	手动开启lazy-free时，有4个选项可以控制：

- `lazyfree-lazy-expire`：key在过期删除时尝试异步释放内存
- `lazyfree-lazy-eviction`：内存达到maxmemory并设置了淘汰策略时尝试异步释放内存
- `lazyfree-lazy-server-del`：执行RENAME/MOVE等命令或需要覆盖一个key时，删除旧key尝试异步释放内存
- `replica-lazy-flush`：主从全量同步，从库清空数据库时异步释放内存

​	**除了replica-lazy-flush之外，其他情况都只是可能去异步释放key的内存，并不是每次必定异步释放内存的。**

​	开启lazy-free后，Redis在释放一个key的内存时，首先会评估代价，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行（不同线程传递数据也会有性能消耗）。

​	a) 当Hash/Set底层采用哈希表存储（非ziplist/int编码存储）时，并且元素数量超过64个
​	b) 当ZSet底层采用跳表存储（非ziplist编码存储）时，并且元素数量超过64个
​	c) 当List链表节点数量超过64个（注意，不是元素数量，而是链表节点的数量，List的实现是在每个节点包含了若干个元素的数据，这些元素采用ziplist存储）

​	也就是说String（不管内存占用多大）、List（少量元素）、Set（int编码存储）、Hash/ZSet（ziplist编码存储）这些情况下的key在释放内存时，依旧在主线程中操作。

​	可见，即使开启了lazy-free，String类型的bigkey，在删除时依旧有阻塞主线程的风险。所以，即便Redis提供了lazy-free，我建议还是尽量不要在Redis中存储bigkey。

​	个人理解Redis在设计评估释放内存的代价时，不是看key的内存占用有多少，而是关注释放内存时的工作量有多大。从上面分析基本能看出，如果需要释放的内存是连续的，Redis作者认为释放内存的代价比较低，就放在主线程做。如果释放的内存不连续（大量指针类型的数据），这个代价就比较高，所以才会放在异步线程中去执行。

## 5.4 查找redis变慢的原因

> 基线性能：一个系统在低压力、无干扰下的基本性能，这个性能只由当前的软硬件配置决定

```shell
# 打印 120 秒内监测到的最大延迟
./redis-cli --intrinsic-latency 120
```

​	如果你想了解网络对 Redis 性能的影响，一个简单的方法是用 **iPerf** 这样的工具，测量从 Redis 客户端到服务器端的网络延迟。如果这个延迟有几十毫秒甚至是几百毫秒，就说明，Redis 运行的网络环境中很可能有大流量的其他应用程序在运行，导致网络拥塞了。这个时候，你就需要协调网络运维，调整网络的流量分配了。

- **【使用复杂度过高的命令（例如SORT/SUION/ZUNIONSTORE/KEYS），或一次查询全量数据（例如LRANGE key 0 N，但N很大）】**

  分析：

  - 查看slowlog是否存在这些命令
  - Redis进程CPU使用率是否飙升（聚合运算命令导致）

  解决：

  - 不使用复杂度过高的命令，或用其他方式代替实现（放在客户端做）
  - 数据尽量分批查询（LRANGE key 0 N，建议N<=100，查询全量数据建议使用HSCAN/SSCAN/ZSCAN）

- **【操作bigkey】**

  分析：

  - slowlog出现很多SET/DELETE变慢命令；
  - 使用`redis-cli -h $host -p $port --bigkeys`扫描出很多bigkey

  解决：优化业务，避免存储bigkey；Redis 4.0+可开启lazy-free机制

- **【大量key集中过期】**

  分析： 业务使用EXPIREAT/PEXPIREAT命令；Redis info中的expired_keys指标短期突增

  解决：优化业务，过期增加随机时间，把时间打散，减轻删除过期key的压力；运维层面，监控`expired_keys`指标，有短期突增及时报警排查

- **【Redis内存达到maxmemory】**

  分析：实例内存达到maxmemory，且写入量大，淘汰key压力变大；Redis info中的`evicted_keys`指标短期突增

  解决：

  - 业务层面，根据情况调整淘汰策略（随机比LRU快） 
  - 运维层面，监控evicted_keys指标，有短期突增及时报警 
  - 集群扩容，多个实例减轻淘汰key的压力

- **【大量短连接请求】**

  分析：Redis处理大量短连接请求，TCP三次握手和四次挥手也会增加耗时

  解决：使用长连接操作Redis

- **【生成RDB和AOF重写fork耗时严重】**

  分析：

  -  Redis变慢只发生在生成RDB和AOF重写期间 
  - 实例占用内存越大，fork拷贝内存页表越久 
  - Redis info中latest_fork_usec耗时变长

  解决：

  - 实例尽量小 
  - Redis尽量部署在物理机上 
  - 优化备份策略（例如低峰期备份）  
  - 合理配置repl-backlog和slave client-output-buffer-limit，避免主从全量同步 
  - 视情况考虑关闭AOF 
  - 监控latest_fork_usec耗时是否变长

- **【AOF使用awalys机制】**

  分析：磁盘IO负载变高

  解决：使用everysec机制；丢失数据不敏感的业务不开启AOF

- **【使用Swap】**

  分析：a) 所有请求全部开始变慢 b) slowlog大量慢日志 c) 查看Redis进程是否使用到了Swap

  解决：a) 增加机器内存 b) 集群扩容 c) Swap使用时监控报警

- **【进程绑定CPU不合理】**

  分析：a) Redis进程只绑定一个CPU逻辑核 b) NUMA架构下，网络中断处理程序和Redis进程没有绑定在同一个Socket下

  解决：a) Redis进程绑定多个CPU逻辑核 b) 网络中断处理程序和Redis进程绑定在同一个Socket下

- **【开启透明大页机制】**

  分析：生成RDB和AOF重写期间，主线程处理写请求耗时变长（拷贝内存副本耗时变长）

  解决：关闭透明大页机制

- **【网卡负载过高】**

  分析：a) TCP/IP层延迟变大，丢包重传变多 b) 是否存在流量过大的实例占满带宽

  解决：a) 机器网络资源监控，负载过高及时报警 b) 提前规划部署策略，访问量大的实例隔离部署

总之，Redis的性能与CPU、内存、网络、磁盘都息息相关，任何一处发生问题，都会影响到Redis的性能。

主要涉及到的包括业务使用层面和运维层面：业务人员需要了解Redis基本的运行原理，使用合理的命令、规避bigke问题和集中过期问题。运维层面需要DBA提前规划好部署策略，预留足够的资源，同时做好监控，这样当发生问题时，能够及时发现并尽快处理。

# 五、缓存

如果需要对写请求进行加速，我们选择读写缓存；

如果写请求很少，或者是只需要提升读请求的响应速度的话，我们选择只读缓存。

## 1. 只读缓存

读请求  --> 缓存  ，缓存缺失再查询数据库，然后回写到缓存

写请求 --> 数据库 --> 删除缓存

<img src="E:\workspace\github\book-mark\images\redis26.jpg" alt="img" style="zoom:25%;" />

​	最新数据都在数据库

## 2. 读写缓存

​	最新数据在redis，有丢失的风险

   同步直写：优先保证数据可靠性

   异步写回：优先提供快速响应，Redis不支持

<img src="E:\workspace\github\book-mark\images\redis27.jpg" alt="img" style="zoom:25%;" />

​		优点：

​		缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力（没有了只读缓存的删除缓存导致缓存缺失和再加载的过程）。

​		缺点：

​		如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据），所以读写缓存比较适合用于读写相当的业务场景。

## 3. 缓存与数据库不一致

<strong style="color:blue">一致性要求:</strong>

- 缓存中有数据，缓存和数据库的数据必须一致
- 缓存中没数据，数据库的值必须是最新值

只读缓存：删改数据时，需要更新数据库，再删除缓存。这两步操作也不是原子性的，也会有一致性问题

同步直写策略：写缓存也写数据库。需要原子性操作，否则有一致性问题

异步写回策略：只写缓存等缓存写满时再写回数据库，如果缓存节点故障，会丢失数据

<strong style="color:blue">一致性问题：</strong>

​	都是由于更新数据库和更新缓存不是原子性造成的

![img](E:\workspace\github\book-mark\images\redis29.jpg)

延迟双删：

​	先删除缓存，再更新数据。不一致情况：

		1. Redis和mysql A=0，线程1写A=1，先删除缓存A，
  		2. 此时线程2读A，发现缓存缺失，读mysql（A=0）,并写入缓存A=0
        		3. 线程1写mysqlA=1
            		4. 此时别的线程读取的就是旧值

  解决：延迟双删，再第3步，写入mysql后，延时再删除一次缓存。但延时的时间无法确定，最好还是使用分布式锁

<strong style="color:blue">具体分析：</strong>

1. 先更新缓存，再更新数据库
   - 读写并发：读到的就是最新值，过段时间数据库数据也会更新，对业务无影响
   - 写写并发：数据库和缓存数据不一致
2. 先更新数据库，再更新缓存
   - 读写并发：读到旧值，但影响时间范围是更新缓存之前，对业务短暂影响
   - 写写并发：数据库和缓存数据不一致

针对无并发情况，对于第二部的更新操作放入消息队列中重试，重试超过一定次数，返回失败给客户端

对于写写并发，需要使用分布式锁。

## 4. 缓存异常

### 4.1 缓存雪崩

> **缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增**。

原因：

​	大量的key再同一时间同时过期，Redis实例不提供服务

解决：

​	避免给大量数据设置相同的过期时间。如果业务要求同时失效，可以增加一些小的随机数

​	主从集群

​	服务降级、服务熔断、请求限流。这些服务治理的手动属于事后补救，

### 4.2 缓存击穿

> 缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。

解决：热点数据不设置有效期

### 4.3 缓存穿透

> 缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。

原因：

​	业务误操作

​	恶意攻击

解决：

​	前端参数验证

​	给不存在的key在缓存中设置默认值

​	使用布隆过滤器判断key是否存在

# 六、淘汰策略

- `noeviction`  不淘汰
- `volatile-random` 过期key随机删除
- `volatile-ttl` 删除快要过期的key
- `volatile-lru` LRU（最近最少使用）
- `volatile-lfu`
- `allkey-random`
- `allkey-lru`
- `allkey-lfu` 

## 6.1 LRU（Least Recently Used）

Redis会记录每个数据最近一次的访问时间戳（RedisObject中的lru字段）

<img src="E:\workspace\github\book-mark\images\redis28.jpg" alt="img" style="zoom:25%;" />

最近最少使用算法。维护了一个链表，只要key被访问，就放在LRU端

LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

简化：

​	在第一次淘汰时，随机选取N个数据作为候选集合，N可以配置，把lru值小的数据淘汰，具体淘汰个数，看当时内存使用情况

​    再次淘汰数据，挑选数据再次进入候选集合，**能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值**，当个数达到了N（`maxmemory-samples`）,再次淘汰lru值小的数据

```shell
# 配置候选集合个数
CONFIG SET maxmemory-samples 100
```

## 6.2 LFU

LRU 策略会在候选数据集中淘汰掉 lru 字段值最小的数据（也就是访问时间最久的数据）

因为只看数据的访问时间，使用 LRU 策略在处理扫描式**单次查询**操作时，无法解决缓存污染

LFU 策略中会从两个维度来筛选并淘汰数据：

- 一是，数据的被访问次数；
- 二是，数据访问的时效性（访问时间离当前时间的远近）。

LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器来统计这个数据的访问次数

LFU是在LRU的基础上实现的

​	RedisObjcet的lru（24bit）字段：前16位：访问时间戳，后8位：访问次数

<strong style="color:blue">计数规则：</strong>

​	8bit最大表示255，所以Redis没有采用数据每被访问一次，就给对应的 counter 值加 1 的计数规则

```shell
# count计数规则
double p = 1.0/(count*lfu_log_factor + 1)
double r = Math.random(0,1)

count = p>r ? count+1 : count;
```

## 6.3 建议

1. **优先使用 allkeys-lru 策略。** 业务数据有明显的冷热区分
2. 没有冷热数据区分，使用`allkey-random`
3. 如果有置顶需求，置顶数据不设置过期时间，并使用`volatile-lru`

# 七、分布式锁

## 7.1 单节点setNx

```shell
setNx lock_key 1 # 加锁
dothings()
del lock_key #解锁
```

1. 在dothings时异常或者解锁失败，导致别的线程永远无法获取锁。给锁设置过期时间

2. 客户端1的锁被客户端2删除了，导致锁失效。

   可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁

   ````shell
   # 加锁, unique_value作为客户端唯一性的标识
   SET lock_key unique_value NX PX 10000
   
   # 解锁需要保证原子性，使用lua脚本
   
   //释放锁 比较unique_value是否相等，避免误释放
   if redis.call("get",KEYS[1]) == ARGV[1] then
       return redis.call("del",KEYS[1])
   else
       return 0
   end
   ````

### 7.2 分布式锁算法 Redlock 

1. 客户端获取当前时间curTime

2. 客户端按顺序依次向N个Redis实例执行加锁操作

   给加锁操作设置超时时间，如果加锁超时，则继续给下一个实例加锁。超时时间 << 锁的有效时间，一般是几十ms

3. 判断加锁是否成功，必须满足以下条件：

   - 超过半数加锁成功
   - 客户端获取锁的总耗时 < 锁的有效时间

4.  如果加锁成功，重新计算这把锁的有效时间，如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况

5. 加锁失败，向所有节点发起释放锁的操作

# 八、命令

```shell
# 关闭aof和rdb
config set appendfsync no 
config set save ""

#设置缓存的最大容量
CONFIG SET maxmemory 4gb

# 从库只读：yes
slave-read-only
```

# 九、配置

过期key的删除：默认每100ms删除一些过期key

```shell
# 1.每过100ms采样20个key，将其中过期的key删除
# 2.如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下。
ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP:20 
```









