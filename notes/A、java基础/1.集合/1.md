![img](E:\workspace\github\book-mark\images\jihe01.png)

# 一、Iterable

```java
public interface Iterable<T> {
    /**
     * Returns an iterator over elements of type {@code T}.
     *
     * @return an Iterator.
     */
    Iterator<T> iterator();
```

- 所有实现了`Collection`的接口的类都可以实现`Iterator`来遍历集合中的对象
- `Iterator`称为迭代器，仅用于遍历，不存放对象
- 增强for就是一个简化版本的迭代器

## 1.1 执行原理

`next()`：指针下移并返回该位置的元素

## 1.2 快速失败

> modCount 记录当前集合被修改的次数

​	当我们使用迭代器或 foreach 遍历时，如果你在 foreach 遍历时，自动调用迭代器的迭代方法，此时在遍历过程中调用了集合的add,remove方法时，modCount就会改变，而迭代器记录的modCount是开始迭代之前的，如果两个不一致，就会报异常，说明有两个线路（线程）同时操作集合。这种操作有风险，为了保证结果的正确性， 避免这样的情况发生，一旦发现modCount与expectedModCount不一致，立即报错。

​	此类的 iterator 和 listIterator 方法返回的迭代器是**快速失败**的：

​		在创建迭代器之后，除非通过迭代器自身的 remove 或 add 方法从结构上对列表进行修改， 否则在任何时间以任何方式对列表进行修改， 迭代器都会抛出 ConcurrentModificationException。 因此，面对并发的修改，迭代器很快就会完全失败， 而不是冒着在将来某个不确定时间发生任意不确定行为的风险。

```java
final void checkForComodification() {
    if (modCount != expectedModCount)
        throw new ConcurrentModificationException();
}
```

# 二、List

## 2.1 ArrayList

数组实现，有序，线程不安全，可重复

默认构造器：

​	初始化数组为{}，首次添加元素`element[]`长度为10，每次扩容1.5倍	

最大容量：Integer.MaxValue

# 三、Map

##  3.1 HashMap

```
DEFAULT_INITIAL_CAPACITY=16; // 默认初始容量
DEFAULT_LOAD_FACTOR=0.75f; // 扩容阈值
TREEIFY_THRESHOLD=8; // 链表的最大长度,当超过该长度时该节点变成红黑树
UNTREEIFY_THRESHOLD=6; // 单个节点红黑树的最小长度，当小于该长度时该节点变成链表
```

### 3.1.1 jdk7和8的区别

- **JDK1.7用的是头插法，而JDK1.8及之后使用的都是尾插法**

<strong style="color:red">头插法多线程添加数据扩容时可能造成链表循环引用</strong>

```java
/**
  * 举例：原来的table:A->B->C->null
  *		线程1准备扩容在indexFor阻塞，此时e=A,next=B
  *     线程2也put数据扩容，transfer执行完毕newTable:C->B->A->null
  *     线程1接着执行，e.next=newTable[i] => A->B
  *                 newTable[i] = e;   => newTable:A->B->A
  *     死循环
  */  
void transfer(Entry[] newTable, boolean rehash) {
        int newCapacity = newTable.length;
        for (Entry<K,V> e : table) {
            while(null != e) {
                Entry<K,V> next = e.next;
                if (rehash) {
                    e.hash = null == e.key ? 0 : hash(e.key);
                }
                // 线程1在此阻塞
                int i = indexFor(e.hash, newCapacity);
                e.next = newTable[i];
                newTable[i] = e;
                e = next;
            }
        }
 }
```

- jdk7数组+链表    jdk8数组+链表+红黑树
- **扩容后数据存储位置的计算方式也不一样：**
  - 1.7 hash值 & length-1
  - 1.8 Hash值的新增参与运算的位是0还是1？0：原始位置；1：原始位置+扩容大小

### 3.1.2 table的长度为什么总是2的幂次？

2的幂次-1：二进制表示尾端都是1，当使用hash & (n-1)，会保留hash后x位

好处：

- **&运算速度快，至少比%取模运算块**
- **能保证 索引值 肯定在 capacity 中，不会超出数组长度**

**尽量避免hash碰撞：**

​	低16位和高16位做异或运算，hash变量只有末x位会参与到运算。使高16位也参与到hash的运算能减少冲突。

````java
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
````

### 3.1.3 为什么 HashMap 中 String、Integer 这样的包装类适合作为 key 键

![150ff30dfa8e8cc71048bc03e4879e56.png](E:\workspace\github\book-mark\images\jihe02.png)

## 3.2 LinkedHashSet

底层是LinkedHashMap，数组+双向链表
Node[pre,after,Entry(next,hash,data,key)]

不能重复

hashCode决定元素的存储位置，同时使用链表维护了元素的次序，看起来元素是以插入顺序保存的 3

 LinkedHashMap在HashMap的基础上多了一个双向链表来维持顺序

## 3.3 HashTable

数组+链表
初始容量11 阈值：0.75
扩容2n+1

线程安全

key和value都不能为空

**Properties是它的子类**

## 3.4 TreeSet、TreeMap

TreeSet底层使用的就是TreeMap

红黑树

使用比较器实现有序插入

## 3.5 ConcurrentHashMap

https://blog.csdn.net/qq_40826814/article/details/115328565

1.8 源码

​	new ConcurrentHashMap(n);

​	sizeCtl = (n+n/2+1) == 2的幂次方 ？  (n+n/2+1)  :  后面的第一个2的幂次

![image-20210627163948419](C:\Users\zzc\AppData\Roaming\Typora\typora-user-images\image-20210627163948419.png)



# 五、ConcurrentHashMap

## 5.1 jdk1.7

jdk1.7中，chm使用分段锁技术，将数据分成一段一段存储，并且对每段数据加锁，当一个线程访问其中一个段的数据时，另外的线程访问其他的分段不受任何影响，在实现并发访问的同时保证了效率。

ConcurrentHashMap中主要实体类就是三个：ConcurrentHashMap（整个Hash表）,Segment（桶），HashEntry（节点）

​	     static final class Segment<K,V> extends ReentrantLock implements Serializable

不变(Immutable)和易变(Volatile)ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁

```
static final class HashEntry<K,V> {
    final int hash;
    final K key;
    volatile V value;
    volatile HashEntry<K,V> next;
}    
```

### 5.1.1 初始化

initialCapacity: 初始容量 默认16
loadFactor： 负载因子 默认0.75f
concurrencyLevel： 并发级别 默认16 用于确认Segment数组的容量，也就是允许同时操作的最大线程数

1. 验证参数的合法性
2. 并发级别不能超过MAX_SEGMENTS 
3. 根据concurrenyLevel计算分段数，分段数ssize总是2的n次幂，计算segmentShift、segmentMask
4. 计算每个Segment平均应该放置多少个元素
5. 最后创建一个Segment实例，将其当做Segment数组的第一个元素

```
public ConcurrentHashMap(int initialCapacity,
                             float loadFactor, int concurrencyLevel) {
        if (!(loadFactor > 0) || initialCapacity < 0 || concurrencyLevel <= 0)
            throw new IllegalArgumentException();
        if (concurrencyLevel > MAX_SEGMENTS)
            concurrencyLevel = MAX_SEGMENTS;
        // Find power-of-two sizes best matching arguments
        int sshift = 0;   //记录sszie向左移位的次数
        int ssize = 1;    // 数组的大小
        while (ssize < concurrencyLevel) {
            ++sshift;
            ssize <<= 1;
        }
        this.segmentShift = 32 - sshift; 
        this.segmentMask = ssize - 1;  // 用于计算hash  2^n-1 低位总是1
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        int c = initialCapacity / ssize;  // 每个Segment的容量
        if (c * ssize < initialCapacity)
            ++c;
        int cap = MIN_SEGMENT_TABLE_CAPACITY;
        while (cap < c)
            cap <<= 1;
        // create segments and segments[0]
        Segment<K,V> s0 =
            new Segment<K,V>(loadFactor, (int)(cap * loadFactor),
                             (HashEntry<K,V>[])new HashEntry[cap]);
        Segment<K,V>[] ss = (Segment<K,V>[])new Segment[ssize];
        UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0]
        this.segments = ss;
    }
```

### 5.1.2 put

1. 检查key值，不允许为null

2. 根据key得到hash，hash(key)将key.hashCode的高位和低位都参与了运算，让hash散列分布

3. 用2的得到的hash右移segmentShift位后与segmentMask与运算，根据hash的高位得到Segment数组的索引

4. 使用Unsafe的方式从Segment数组中获取该索引对应的Segment对象

5. put中使用lock锁住整个Segment，然后根据hash= (tab.length - 1) & hash 算出该key在Segment中的位置

   然后使用头插法插入数据

```
public V put(K key, V value) {
    Segment<K,V> s;
    if (value == null)
        throw new NullPointerException();
    int hash = hash(key);
    int j = (hash >>> segmentShift) & segmentMask;
    if ((s = (Segment<K,V>)UNSAFE.getObject          // nonvolatile; recheck
         (segments, (j << SSHIFT) + SBASE)) == null) //  in ensureSegment
        s = ensureSegment(j);
    return s.put(key, hash, value, false);
}
```

### 5.1.3 get

1. 和put操作一样，先通过key进行两次hash确定应该去哪个Segment中取数据。
2. 使用Unsafe获取对应的Segment，然后再进行一次&运算得到HashEntry链表的位置，然后从链表头开始遍历整个链表（因为Hash可能会有碰撞，所以用一个链表保存），如果找到对应的key，则返回对应的value值，如果链表遍历完都没有找到对应的key，则说明Map中不包含该key，返回null。

值得注意的是，get操作是不需要加锁的（如果value为null，会调用readValueUnderLock，只有这个步骤会加锁），通过前面提到的volatile和final来确保数据安全。

### 5.1.4 size、containsValue操作

size操作与put和get操作最大的区别在于，size操作需要遍历所有的Segment才能算出整个Map的大小，而put和get都只关心一个Segment。假设我们当前遍历的Segment为SA，那么在遍历SA过程中其他的Segment比如SB可能会被修改，于是这一次运算出来的size值可能并不是Map当前的真正大小。所以一个比较简单的办法就是计算Map大小的时候所有的Segment都Lock住，不能更新(包含put，remove等等)数据，计算完之后再Unlock。这是普通人能够想到的方案，但是牛逼的作者还有一个更好的Idea：先给3次机会，不lock所有的Segment，遍历所有Segment，累加各个Segment的大小得到整个Map的大小，如果某相邻的两次计算获取的所有Segment的更新的次数（每个Segment都有一个modCount变量，这个变量在Segment中的Entry被修改时会加一，通过这个值可以得到每个Segment的更新操作的次数）是一样的，说明计算过程中没有更新操作，则直接返回这个值。如果这三次不加锁的计算过程中Map的更新次数有变化，则之后的计算先对所有的Segment加锁，再遍历所有Segment计算Map大小

## 5.2 jdk1.8

>数组+链表+红黑树
>
>sizectl
>	  `>0 && 数组为空`，表示初始化数组容量大小
> `>0 && 数组不为空`，表示扩容阈值
> `=-1` 数组正在初始化
> `<0 && !=-n` 数组正在扩容，并且有n-1个线程正在辅助扩容

 >**ForwardingNode**
 >
 >一个用于连接两个table的节点类。它包含一个nextTable指针，用于指向下一张表。而且这个节点的key value next指针全部为null，它的hash值为-1

### 5.2.1 初始化参数

在new实例传参时，不会初始化数组，数组容量大小:  参数 * 2

```
public ConcurrentHashMap(int initialCapacity) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException();
        int cap = ((initialCapacity >= (MAXIMUM_CAPACITY >>> 1)) ?
                   MAXIMUM_CAPACITY :
                   tableSizeFor(initialCapacity + (initialCapacity >>> 1) + 1));
        this.sizeCtl = cap;
    }
```

### 5.2.2 put

1. 得到key的hash值，开始自旋，直到插入成功
2. 判断table==null
3. 是则初始化数组（initTable），然后继续2， 否则执行4
4. 根据hash得到插入位置的索引，判断该索引的值是否为null，如果为null，执行5，否则执行6
5. 使用cas插入元素，如果插入失败，意味着别的线程已经改变了当前位置的值，需要重新开始2；否则插入成功执行 8
6. 判断当前位置的值是否为MOVED，是：当前数组正在扩容，需要辅助扩容；否：执行7
7. 上述情况全部排除后，需要锁住该节点，然后插入到链表或者树中
8. 1-7自旋插入元素成功后，计数+1



![image-20201214105300676](E:\workspace\github\book-mark\images\map04.png)

```
final V putVal(K key, V value, boolean onlyIfAbsent) {
        if (key == null || value == null) throw new NullPointerException();
        int hash = spread(key.hashCode());
        int binCount = 0;
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                tab = initTable();
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                if (casTabAt(tab, i, null,
                             new Node<K,V>(hash, key, value, null)))
                    break;                   // no lock when adding to empty bin
            }
            else if ((fh = f.hash) == MOVED)
                tab = helpTransfer(tab, f);
            else {
                V oldVal = null;
                synchronized (f) {
                    if (tabAt(tab, i) == f) {
                        if (fh >= 0) {
                            binCount = 1;
                            for (Node<K,V> e = f;; ++binCount) {
                                K ek;
                                if (e.hash == hash &&
                                    ((ek = e.key) == key ||
                                     (ek != null && key.equals(ek)))) {
                                    oldVal = e.val;
                                    if (!onlyIfAbsent)
                                        e.val = value;
                                    break;
                                }
                                Node<K,V> pred = e;
                                if ((e = e.next) == null) {
                                    pred.next = new Node<K,V>(hash, key,
                                                              value, null);
                                    break;
                                }
                            }
                        }
                        else if (f instanceof TreeBin) {
                            Node<K,V> p;
                            binCount = 2;
                            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                           value)) != null) {
                                oldVal = p.val;
                                if (!onlyIfAbsent)
                                    p.val = value;
                            }
                        }
                    }
                }
                if (binCount != 0) {
                    if (binCount >= TREEIFY_THRESHOLD)
                        treeifyBin(tab, i);
                    if (oldVal != null)
                        return oldVal;
                    break;
                }
            }
        }
        addCount(1L, binCount);
        return null;
    }
```



## 5.3 区别

在 JDK1.7 中，ConcurrentHashMap 采用了分段锁策略，将一个 HashMap 切割成 Segment 数组，其中 Segment 可以看成一个 HashMap， 不同点是 Segment 继承自 ReentrantLock，在操作的时候给 Segment 赋予了一个对象锁，从而保证多线程环境下并发操作安全。

但是 JDK1.7 中，**HashMap 容易因为冲突链表过长，造成查询效率低**，所以在 JDK1.8 中，HashMap 引入了红黑树特性，当冲突链表长度大于 8 时，会将链表转化成红黑二叉树结构。

在 JDK1.8 中，与此对应的 ConcurrentHashMap 也是采用了与 HashMap 类似的存储结构，但是 JDK1.8 中 ConcurrentHashMap 并没有采用分段锁的策略，而是在元素的节点上采用 `CAS + synchronized` 操作来保证并发的安全性，源码的实现比 JDK1.7 要复杂的多。



















